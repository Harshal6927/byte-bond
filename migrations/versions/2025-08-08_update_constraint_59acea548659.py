"""update-constraint

Revision ID: 59acea548659
Revises: 2ddc40e70b73
Create Date: 2025-08-08 02:47:28.924653

"""

import warnings
from typing import TYPE_CHECKING

import sqlalchemy as sa
from alembic import op
from advanced_alchemy.types import EncryptedString, EncryptedText, GUID, ORA_JSONB, DateTimeUTC, StoredObject, PasswordHash
from sqlalchemy import Text  # noqa: F401
from sqlalchemy.dialects import postgresql
if TYPE_CHECKING:
    from collections.abc import Sequence

__all__ = ["downgrade", "upgrade", "schema_upgrades", "schema_downgrades", "data_upgrades", "data_downgrades"]

sa.GUID = GUID
sa.DateTimeUTC = DateTimeUTC
sa.ORA_JSONB = ORA_JSONB
sa.EncryptedString = EncryptedString
sa.EncryptedText = EncryptedText
sa.StoredObject = StoredObject

# revision identifiers, used by Alembic.
revision = '59acea548659'
down_revision = '2ddc40e70b73'
branch_labels = None
depends_on = None


def upgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            schema_upgrades()
            data_upgrades()

def downgrade() -> None:
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=UserWarning)
        with op.get_context().autocommit_block():
            data_downgrades()
            schema_downgrades()

def schema_upgrades() -> None:
    """schema upgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table('saq_versions')
    with op.batch_alter_table('saq_stats', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('saq_stats_expire_at_idx'))
        batch_op.drop_index(batch_op.f('saq_stats_queue_key_idx'))

    op.drop_table('saq_stats')
    with op.batch_alter_table('saq_jobs', schema=None) as batch_op:
        batch_op.drop_index(batch_op.f('saq_jobs_status_queue_group_key_idx'))
        batch_op.drop_index(batch_op.f('saq_jobs_status_queue_priority_scheduled_idx'))

    op.drop_table('saq_jobs')
    with op.batch_alter_table('connection_questions', schema=None) as batch_op:
        batch_op.drop_constraint(batch_op.f('uq_connection_question'), type_='unique')
        batch_op.create_unique_constraint('uq_connection_question', ['connection_id', 'question_id', 'user_id'])

    # ### end Alembic commands ###

def schema_downgrades() -> None:
    """schema downgrade migrations go here."""
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table('connection_questions', schema=None) as batch_op:
        batch_op.drop_constraint('uq_connection_question', type_='unique')
        batch_op.create_unique_constraint(batch_op.f('uq_connection_question'), ['connection_id', 'question_id'], postgresql_nulls_not_distinct=False)

    op.create_table('saq_jobs',
    sa.Column('key', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('lock_key', sa.INTEGER(), autoincrement=True, nullable=False),
    sa.Column('job', postgresql.BYTEA(), autoincrement=False, nullable=False),
    sa.Column('queue', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('status', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('priority', sa.SMALLINT(), server_default=sa.text('0'), autoincrement=False, nullable=False),
    sa.Column('group_key', sa.TEXT(), autoincrement=False, nullable=True),
    sa.Column('scheduled', sa.BIGINT(), server_default=sa.text('EXTRACT(epoch FROM now())'), autoincrement=False, nullable=False),
    sa.Column('expire_at', sa.BIGINT(), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('key', name=op.f('saq_jobs_pkey'))
    )
    with op.batch_alter_table('saq_jobs', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('saq_jobs_status_queue_priority_scheduled_idx'), ['status', 'queue', 'priority', 'scheduled'], unique=False)
        batch_op.create_index(batch_op.f('saq_jobs_status_queue_group_key_idx'), ['status', 'queue', 'group_key'], unique=False)

    op.create_table('saq_stats',
    sa.Column('worker_id', sa.TEXT(), autoincrement=False, nullable=False),
    sa.Column('stats', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('expire_at', sa.BIGINT(), autoincrement=False, nullable=True),
    sa.Column('metadata', postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),
    sa.Column('queue_key', sa.TEXT(), autoincrement=False, nullable=True),
    sa.PrimaryKeyConstraint('worker_id', name=op.f('saq_stats_pkey'))
    )
    with op.batch_alter_table('saq_stats', schema=None) as batch_op:
        batch_op.create_index(batch_op.f('saq_stats_queue_key_idx'), ['queue_key'], unique=False)
        batch_op.create_index(batch_op.f('saq_stats_expire_at_idx'), ['expire_at'], unique=False)

    op.create_table('saq_versions',
    sa.Column('version', sa.INTEGER(), autoincrement=False, nullable=True)
    )
    # ### end Alembic commands ###

def data_upgrades() -> None:
    """Add any optional data upgrade migrations here!"""

def data_downgrades() -> None:
    """Add any optional data downgrade migrations here!"""
